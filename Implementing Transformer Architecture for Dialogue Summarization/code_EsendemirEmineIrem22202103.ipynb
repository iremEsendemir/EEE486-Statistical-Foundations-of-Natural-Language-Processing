{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZTZCjBThxgs",
        "outputId": "26284dab-1413-4d8d-bba9-4b0c93e2d49b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zXpavNYCula4",
        "outputId": "2703aaa4-bb2c-437e-d6eb-9b279ae5903f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m894.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install bert-score  # Install the necessary library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFrpCt0yh1eQ"
      },
      "source": [
        "\n",
        "# Transformer Summarizer\n",
        "\n",
        "In this assignment you will explore summarization using the transformer model. **You will be implementing an encoder-decoder model. Don't worry; you will be guided through all the steps, and you will find numerous hints to assist you!**\n",
        "\n",
        "There are many hints in this notebook so feel free to use them as needed. Actually by the end of this notebook you will have implemented the full transformer (both encoder and decoder).\n",
        "\n",
        "You will only need to write code in between the parts you see:    \\\n",
        "START CODE HERE     \\\n",
        "... \\\n",
        "... \\\n",
        "END CODE HERE  \\\n",
        "\n",
        "You might need to make some small changes in other areas, like updating file paths and resolving issues with package version dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3fIQ3gBnG51"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#0)\n",
        "- [1 - Importing the Dataset](#1)\n",
        "- [2 - Preprocess the Data](#2)\n",
        "- [3 - Positional Encoding](#3)\n",
        "- [4 - Masking](#4)\n",
        "- [5 - Self-attention](#5)\n",
        "    - [Exercise 1 - scaled_dot_product_attention](#ex-1)\n",
        "- [6 - Encoder](#6)\n",
        "    - [6.1 - Encoder Layer](#6-1)\n",
        "    - [6.2 - Full Encoder](#6-2)\n",
        "- [7 - Decoder](#7)\n",
        "    - [7.1 - Decoder Layer](#7-1)\n",
        "    - [Exercise 2 - DecoderLayer](#ex-2)\n",
        "    - [7.2 - Full Decoder](#7-2)\n",
        "    - [Exercise 3 - Decoder](#ex-3)\n",
        "- [8 - Transformer](#8)\n",
        "    - [Exercise 4 - Transformer](#ex-4)\n",
        "- [9 - Initialize the Model](#9)\n",
        "- [10 - Prepare for Training the Model](#10)\n",
        "- [11 - Summarization](#11)\n",
        "    - [Exercise 5 - next_word](#ex-5)\n",
        "- [12 - Train the Model](#12)\n",
        "- [13 - Summarize some sentences!](#13)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtpuFSwLnLsR"
      },
      "source": [
        "<a name='0'></a>\n",
        "## Introduction\n",
        "\n",
        "Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Who wants to read an article or a long email today anyway, when you can build a transformer to summarize text for you? Let's get started. By completing this assignment you will learn to:  \n",
        "\n",
        "- Use built-in functions to preprocess your data\n",
        "- Implement DotProductAttention\n",
        "- Implement Causal Attention\n",
        "- Understand how attention works\n",
        "- Build the transformer model\n",
        "- Evaluate your model\n",
        "- Summarize an article\n",
        "\n",
        "As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g-TAYeglWH1",
        "outputId": "d78b4205-1e02-44c9-df52-fcc8d169f50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'gdrive', 'sample_data']\n",
            "Current Directory: /content/gdrive/My Drive/NLP/transformer/final_version\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir())\n",
        "\n",
        "os.chdir(\"/content/gdrive/My Drive/NLP/transformer/final_version\")\n",
        "print(\"Current Directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoH5aMFvh9bE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "# import utils\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtFhK5hI9CXd"
      },
      "outputs": [],
      "source": [
        "def get_train_test_data(data_dir):\n",
        "    # Get the train data\n",
        "    train_data = pd.read_json(f\"{data_dir}/dialogsum.train.jsonl\", lines=True)\n",
        "    train_data.drop(['fname'], axis=1, inplace=True)\n",
        "    train_data.drop(['topic'], axis=1, inplace=True)\n",
        "\n",
        "    # Get the test data\n",
        "    test_data = pd.read_json(f\"{data_dir}/dialogsum.test.jsonl\", lines=True)\n",
        "    test_data.drop(['fname'], axis=1, inplace=True)\n",
        "    # Drop columns starting with \"topic\" (e.g., topic1, topic2, topic3, etc.)\n",
        "    topic_columns = [col for col in test_data.columns if col.startswith('topic')]\n",
        "    test_data.drop(columns=topic_columns, inplace=True)\n",
        "\n",
        "    # Combine summary columns into a single 'summary' column\n",
        "    summary_cols = [col for col in test_data.columns if col.startswith('summary')]\n",
        "    test_data['summary'] = test_data[summary_cols].apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
        "    test_data = test_data[['dialogue', 'summary']] # keep only dialogue and summary\n",
        "\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "def preprocess(input_data):\n",
        "    # Define the custom preprocessing function\n",
        "    def preprocess_util(input_data):\n",
        "        # Convert all text to lowercase\n",
        "        lowercase = input_data.lower()\n",
        "        # Remove newlines and double spaces\n",
        "        removed_newlines = re.sub(\"\\n|\\r|\\t\", \" \",  lowercase)\n",
        "        removed_double_spaces = ' '.join(removed_newlines.split(' '))\n",
        "        # Add start of sentence and end of sentence tokens\n",
        "        s = '[SOS] ' + removed_double_spaces + ' [EOS]'\n",
        "        return s\n",
        "\n",
        "    # Apply the preprocessing to the train and test datasets\n",
        "    input_data['summary'] = input_data.apply(lambda row : preprocess_util(row['summary']), axis = 1)\n",
        "    input_data['dialogue'] = input_data.apply(lambda row : preprocess_util(row['dialogue']), axis = 1)\n",
        "\n",
        "    document = input_data['dialogue']\n",
        "    summary = input_data['summary']\n",
        "\n",
        "    return document, summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35KWXJwsnQY0"
      },
      "source": [
        "<a name='1'></a>\n",
        "## PART A.1 - Import the Dataset\n",
        "You have the dataset saved in a .jsonl file. The loading function has already been taken care of in `get_train_test_data()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UDVR_LYnTSz",
        "outputId": "5ed1cedf-4efe-4713-a7c5-2876041aac53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue:\n",
            "#Person1#: Could you do me a favor?\n",
            "#Person2#: Sure. What is it?\n",
            "#Person1#: Could you run over to the store? We need a few things.\n",
            "#Person2#: All right. What do you want me to get?\n",
            "#Person1#: Well, could you pick up some sugar?\n",
            "#Person2#: Okay. How much?\n",
            "#Person1#: A small bag. I guess we also need a few oranges.\n",
            "#Person2#: How many?\n",
            "#Person1#: Oh, let's see. . . About six.\n",
            "#Person2#: Anything else?\n",
            "#Person1#: Yes. We're out of milk.\n",
            "#Person2#: Okay. How much do you want me to get? A gallon?\n",
            "#Person1#: No. I think a half gallon will be enough.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: I think so. Have you got all that?\n",
            "#Person2#: Yes. That's small bag of sugar, four oranges, and a half gallon of milk.\n",
            "#Person1#: Do you have enough money?\n",
            "#Person2#: I think so.\n",
            "#Person1#: Thanks very much. I appreciate it.\n",
            "\n",
            "Summary:\n",
            "#Person1# asks #Person2# to do a favor. #Person2# agrees and helps buy a small bag of sugar, six oranges, and a half-gallon of milk.\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"data/corpus2\"\n",
        "\n",
        "train_data, test_data = get_train_test_data(data_dir)\n",
        "\n",
        "# Take one example from the dataset and print it\n",
        "example_dialogue, example_summary = train_data.iloc[10]\n",
        "print(f\"Dialogue:\\n{example_dialogue}\")\n",
        "print(f\"\\nSummary:\\n{example_summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa01Fq0onb7H"
      },
      "source": [
        "<a name='2'></a>\n",
        "## Preprocess the data\n",
        "\n",
        "First you will do some preprocessing of the data and split it into inputs and outputs. Here you also remove some of the characters that are specific to this dataset and add the `[EOS]` (end of sentence) token to the end, like it was discussed in the lecture videos. You will also add a `[SOS]` (start of sentence) token to the beginning of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oHH--bcneOh"
      },
      "outputs": [],
      "source": [
        "document, summary = preprocess(train_data)\n",
        "document_test, summary_test = preprocess(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jsltUwgnjuE"
      },
      "source": [
        "Now perform the standard preprocessing with the tensorflow library. You will need to modify the filters, because you dont want the `[EOS]` tokens to be removed.\n",
        "\n",
        "Then create the vocabulary by combining the data in the documents and the summaries and using `.fit_on_texts()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXnxYLo1nhcj",
        "outputId": "4cd146f1-4fc6-41a1-ddca-551afc5fa248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary: 22901\n"
          ]
        }
      ],
      "source": [
        "# The [ and ] from default tokens cannot be removed, because they mark the SOS and EOS token.\n",
        "filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'\n",
        "oov_token = '[UNK]'\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token, lower=False)\n",
        "\n",
        "documents_and_summary = pd.concat([document, summary], ignore_index=True)\n",
        "\n",
        "tokenizer.fit_on_texts(documents_and_summary)\n",
        "\n",
        "inputs = tokenizer.texts_to_sequences(document)\n",
        "targets = tokenizer.texts_to_sequences(summary)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f'Size of vocabulary: {vocab_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oex1prjxnslg"
      },
      "source": [
        "Now you can pad the tokenized sequences for the training data.\n",
        "\n",
        "For the purpose of this notebook you need to limit the length of the sequences, as transformers are really big models and are not meant to be trained in such small environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYIn7fyvnqLE"
      },
      "outputs": [],
      "source": [
        "# Limit the size of the input and output data for being able to run it in this environment.\n",
        "encoder_maxlen = 150\n",
        "decoder_maxlen = 50\n",
        "\n",
        "# Pad the sequences.\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)\n",
        "\n",
        "# Create the final training dataset.\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-BO8DOAnzBN"
      },
      "source": [
        "<a name='3'></a>\n",
        "## Positional Encoding\n",
        "\n",
        "In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model. However, when you train a Transformer network using multi-head attention, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful.\n",
        "\n",
        "You have learned how to implement the positional encoding in one of this week's labs. Here you will use the `positional_encoding` function to create positional encodings for your transformer. The function is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKQ5hXJsnz_-"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(positions, d_model):\n",
        "    \"\"\"\n",
        "    Precomputes a matrix with all the positional encodings\n",
        "\n",
        "    Arguments:\n",
        "        positions (int): Maximum number of positions to be encoded\n",
        "        d_model (int): Encoding size\n",
        "\n",
        "    Returns:\n",
        "        pos_encoding (tf.Tensor): A matrix of shape (1, position, d_model) with the positional encodings\n",
        "    \"\"\"\n",
        "\n",
        "    position = np.arange(positions)[:, np.newaxis]\n",
        "    k = np.arange(d_model)[np.newaxis, :]\n",
        "    i = k // 2\n",
        "\n",
        "    # initialize a matrix angle_rads of all the angles\n",
        "    angle_rates = 1 / np.power(10000, (2 * i) / np.float32(d_model))\n",
        "    angle_rads = position * angle_rates\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50XVFoJAn4Ib"
      },
      "source": [
        "<a name='4'></a>\n",
        "## Masking\n",
        "\n",
        "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence.\n",
        "\n",
        "You have already learned how to implement and use them in one of this week's labs. Here they are implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTI_fVYRn6bb"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(decoder_token_ids):\n",
        "    \"\"\"\n",
        "    Creates a matrix mask for the padding cells\n",
        "\n",
        "    Arguments:\n",
        "        decoder_token_ids (matrix like): matrix of size (n, m)\n",
        "\n",
        "    Returns:\n",
        "        mask (tf.Tensor): binary tensor of size (n, 1, m)\n",
        "    \"\"\"\n",
        "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding to the attention logits.\n",
        "    # this will allow for broadcasting later when comparing sequences\n",
        "    return seq[:, tf.newaxis, :]\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(sequence_length):\n",
        "    \"\"\"\n",
        "    Returns a lower triangular matrix filled with ones\n",
        "\n",
        "    Arguments:\n",
        "        sequence_length (int): matrix size\n",
        "\n",
        "    Returns:\n",
        "        mask (tf.Tensor): binary tensor of size (sequence_length, sequence_length)\n",
        "    \"\"\"\n",
        "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FbqUpteoA8V"
      },
      "source": [
        "<a name='5'></a>\n",
        "## PART A.2 - Self-Attention\n",
        "\n",
        "As the authors of the Transformers paper state, \"Attention is All You Need\".\n",
        "    \n",
        "The use of self-attention paired with traditional convolutional networks allows for parallelization which speeds up training. You will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to return rich, attention-based vector representations of the words in your sequence.\n",
        "\n",
        "Implement the function `scaled_dot_product_attention()` to create attention-based representations.\n",
        "\n",
        "**Reminder**: The boolean mask parameter can be passed in as `none` or as either padding or look-ahead.\n",
        "    \n",
        "* Multiply (1. - mask) by -1e9 before adding it to the scaled attention logits.\n",
        "\n",
        "**Additional Hints**\n",
        "* You may find [tf.matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) useful for matrix multiplication (check how you can use the parameter transpose_b).\n",
        "* You can use [tf.keras.activations.softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) for softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIhN0HiCoDq1"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: scaled_dot_product_attention\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "      q, k, v must have matching leading dimensions.\n",
        "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "      The mask has different shapes depending on its type(padding or look ahead)\n",
        "      but it must be broadcastable for addition.\n",
        "\n",
        "    Arguments:\n",
        "        q (tf.Tensor): query of shape (..., seq_len_q, depth)\n",
        "        k (tf.Tensor): key of shape (..., seq_len_k, depth)\n",
        "        v (tf.Tensor): value of shape (..., seq_len_v, depth_v)\n",
        "        mask (tf.Tensor): mask with shape broadcastable\n",
        "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        output -- attention_weights\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Multiply q and k transposed.\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk with the square root of dk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:  # Don't replace this None\n",
        "        scaled_attention_logits += (1 - mask) * -1e9\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "    attention_weights = tf.keras.activations.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # Multiply the attention weights by v\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn55XxQpoIcT",
        "outputId": "71a93069-d3df-47c1-e613-7f34b572368b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            " [[[0.5  0.75]\n",
            "  [0.31 0.84]\n",
            "  [0.27 1.  ]]]\n",
            "\n",
            "Attention weigths:\n",
            " [[[0.25 0.25 0.   0.25 0.25]\n",
            "  [0.43 0.   0.16 0.16 0.26]\n",
            "  [0.45 0.   0.   0.27 0.27]]]\n"
          ]
        }
      ],
      "source": [
        "# Test your function!\n",
        "q = np.array([[0, 0, 0, 1], [0, 1, 1, 1], [1, 0, 1, 1]]).astype(np.float32)\n",
        "k = np.array([[0, 1, 1, 1], [1, 0, 0, 1 ], [1, 0, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]).astype(np.float32)\n",
        "v = np.array([[0, 1], [1, 0], [1, 0], [1, 1], [0, 1]]).astype(np.float32)\n",
        "mask = np.array([[[1, 1, 0, 1, 1], [1, 0, 1, 1, 1], [1, 0, 0, 1, 1]]])\n",
        "\n",
        "ou, atw = scaled_dot_product_attention(q, k, v, mask)\n",
        "ou = np.around(ou, decimals=2)\n",
        "atw = np.around(atw, decimals=2)\n",
        "\n",
        "print(f\"Output:\\n {ou}\")\n",
        "print(f\"\\nAttention weigths:\\n {atw}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gJ1brQ-oX_5"
      },
      "source": [
        "\\Excellent work! You can now implement self-attention. With that, you can start building the encoder block!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7_q_LPKoTo2"
      },
      "source": [
        "<a name='6'></a>\n",
        "## PART A.3 - Encoder\n",
        "\n",
        "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which you'll build later in the assignment. In this section of the assignment, you will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a).\n",
        "\n",
        "\n",
        "* `MultiHeadAttention` you can think of as computing the self-attention several times to detect different features.\n",
        "* Feed forward neural network contains two Dense layers which we'll implement as the function `FullyConnected`\n",
        "\n",
        "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n",
        "   \n",
        "* For the `MultiHeadAttention` layer, you will use the [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) implemented in Keras. If you're curious about how to split the query matrix Q, key matrix K, and value matrix V into different heads, you can look through the implementation.\n",
        "* You will also use the [Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) with two dense layers to built the feed forward neural network layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAIStBPooc8a"
      },
      "outputs": [],
      "source": [
        "def FullyConnected(embedding_dim, fully_connected_dim):\n",
        "    \"\"\"\n",
        "    Returns a sequential model consisting of two dense layers. The first dense layer has\n",
        "    fully_connected_dim neurons and is activated by relu. The second dense layer has\n",
        "    embedding_dim and no activation.\n",
        "\n",
        "    Arguments:\n",
        "        embedding_dim (int): output dimension\n",
        "        fully_connected_dim (int): dimension of the hidden layer\n",
        "\n",
        "    Returns:\n",
        "        _ (tf.keras.Model): sequential model\n",
        "    \"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, d_model)\n",
        "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZxMzf1Hoj9U"
      },
      "source": [
        "<a name='6-1'></a>\n",
        "### Encoder Layer\n",
        "\n",
        "Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training (Figure 2a).\n",
        "\n",
        "The encoder block (Figure 2) is is already implemented for you. Take a very close look at its implementation, as you will later have to create the decoder yourself, and a lot of the code is very similar. The encoder block performs the following steps:\n",
        "1. It takes the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute *self*-attention Q, V and K are the same. You will also perform Dropout in this multi-head attention layer during training.\n",
        "2. There is a skip connection to add your original input `x` and the output of the multi-head attention layer.\n",
        "3. After adding the skip connection, the output passes through the first normalization layer.\n",
        "4. Finally, steps 1-3 are repeated but with the feed forward neural network with a dropout layer instead of the multi-head attention layer.\n",
        "\n",
        "<details>\n",
        "  <summary><font size=\"2\" color=\"darkgreen\"><b>Additional Information (Click to expand)</b></font></summary>\n",
        "    \n",
        "* The `__init__` method creates all the layers that will be accesed by the the `call` method. Wherever you want to use a layer defined inside  the `__init__`  method you will have to use the syntax `self.[insert layer name]`.\n",
        "* You will find the documentation of [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) helpful. *Note that if query, key and value are the same, then this function performs self-attention.*\n",
        "* The call arguments for `self.mha` are (Where B is for batch_size, T is for target sequence shapes, and S is output_shape):\n",
        " - `query`: Query Tensor of shape (B, T, dim).\n",
        " - `value`: Value Tensor of shape (B, S, dim).\n",
        " - `key`: Optional key Tensor of shape (B, S, dim). If not given, will use the same value for both key and value, which is the most common case.\n",
        " - `attention_mask`: a boolean mask of shape (B, T, S), that prevents attention to certain positions. The boolean mask specifies which query elements can attend to which key elements, 1 indicates attention and 0 indicates no attention. Broadcasting can happen for the missing batch dimensions and the head dimension.\n",
        " - `return_attention_scores`: A boolean to indicate whether the output should be attention output if True, or (attention_output, attention_scores) if False. Defaults to False.\n",
        " - `training`: Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (no dropout). Defaults to either using the training mode of the parent layer/model, or False (inference) if there is no parent layer. Take a look at [tf.keras.layers.Dropout](https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/Dropout) for more details (Additional reading in [Keras FAQ](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHKB5nQGonQL"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
        "    followed by a simple, positionwise fully connected feed-forward network.\n",
        "    This architecture includes a residual connection around each of the two\n",
        "    sub-layers, followed by layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
        "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dim,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.ffn = FullyConnected(\n",
        "            embedding_dim=embedding_dim,\n",
        "            fully_connected_dim=fully_connected_dim\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder Layer\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "        Returns:\n",
        "            encoder_layer_out (tf.Tensor): Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        # calculate self-attention using mha(~1 line).\n",
        "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
        "        self_mha_output = self.mha(x, x, x, mask)  # Inputs are key, query, values, and mask. Self attention (batch_size, input_seq_len, fully_connected_dim)\n",
        "\n",
        "        # skip connection\n",
        "        # apply layer normalization on sum of the input and the attention output to get the\n",
        "        # output of the multi-head attention layer\n",
        "        skip_x_attention = self.layernorm1(x + self_mha_output)  # (batch_size, input_seq_len, fully_connected_dim)\n",
        "\n",
        "        # pass the output of the multi-head attention layer through a ffn\n",
        "        ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, fully_connected_dim)\n",
        "\n",
        "        # apply dropout layer to ffn output during training\n",
        "        # use `training=training`\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "\n",
        "        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output\n",
        "        # to get the output of the encoder layer\n",
        "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "\n",
        "        return encoder_layer_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPXy0TSqotYt"
      },
      "source": [
        "<a name='6-2'></a>\n",
        "### Full Encoder\n",
        "\n",
        "Now you're ready to build the full Transformer Encoder (Figure 2b), where you will embed your input and add the positional encodings you calculated. You will then feed your encoded embeddings to a stack of Encoder layers.\n",
        "\n",
        "\n",
        "\n",
        "The Encoder class is implemented for you. It performs the following steps:\n",
        "1. Pass the input through the Embedding layer.\n",
        "2. Scale the embedding by multiplying it by the square root of the embedding dimension.\n",
        "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to the embedding.\n",
        "4. Pass the encoded embedding through a dropout layer\n",
        "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MIYm0IDoxix"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the input to an embedding layer\n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    encoder Layers\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                                self.embedding_dim)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps)\n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, seq_len)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "\n",
        "        Returns:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, seq_len, embedding dim)\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Pass input through the Embedding layer\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
        "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "        # Add the position encoding to embedding\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        # Pass the encoded embedding through a dropout layer\n",
        "        # use `training=training`\n",
        "        x = self.dropout(x, training=training)\n",
        "        # Pass the output through the stack of encoding layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCfLkbtQONR4",
        "outputId": "22accc93-a29f-4306-f06e-116ead542155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using embedding_dim=16, num_heads=4, num_layers=2:\n",
            "\n",
            "Input shape: (1, 10)\n",
            "Mask shape: (1, 1, 1, 10)\n",
            "Output of encoder has shape: (1, 10, 16)\n"
          ]
        }
      ],
      "source": [
        "# Test your function!\n",
        "\n",
        "# Instantiate and test Encoder\n",
        "embedding_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "fully_connected_dim = 32\n",
        "input_vocab_size = 500\n",
        "maximum_position_encoding = 20\n",
        "\n",
        "# Create test input\n",
        "batch_size = 1\n",
        "seq_len = 10\n",
        "test_input = np.ones((batch_size, seq_len))  # Input token IDs (all ones for simplicity)\n",
        "\n",
        "# Convert to tensor\n",
        "test_input_tensor = tf.convert_to_tensor(test_input, dtype=tf.int32)\n",
        "\n",
        "# Dummy mask (e.g., padding mask with all ones = no padding)\n",
        "test_mask = tf.ones((batch_size, 1, 1, seq_len))\n",
        "\n",
        "# Initialize Encoder\n",
        "encoder_test = Encoder(num_layers=num_layers,\n",
        "                       embedding_dim=embedding_dim,\n",
        "                       num_heads=num_heads,\n",
        "                       fully_connected_dim=fully_connected_dim,\n",
        "                       input_vocab_size=input_vocab_size,\n",
        "                       maximum_position_encoding=maximum_position_encoding)\n",
        "\n",
        "# Forward pass\n",
        "encoder_output = encoder_test(test_input_tensor, training=False, mask=test_mask)\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Using embedding_dim={embedding_dim}, num_heads={num_heads}, num_layers={num_layers}:\\n\")\n",
        "print(f\"Input shape: {test_input_tensor.shape}\")\n",
        "print(f\"Mask shape: {test_mask.shape}\")\n",
        "print(f\"Output of encoder has shape: {encoder_output.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb-M4F-lov_T"
      },
      "source": [
        "<a name='7'></a>\n",
        "## PART A.4 - Decoder\n",
        "\n",
        "Now it is time to implement the decoder. You have seen it in the videos and you can use some help by looking at the encoder implementation above. The Decoder layer takes the K and V matrices generated by the Encoder and computes the second multi-head attention layer with the Q matrix from the output (Figure 3a).\n",
        "\n",
        "\n",
        "\n",
        "<a name='7-1'></a>    \n",
        "### Decoder Layer\n",
        "Again, you'll pair multi-head attention with a feed forward neural network, but this time you'll implement two multi-head attention layers. You will also use residual connections and layer normalization to help speed up training (Figure 3a).\n",
        "\n",
        "<a name='ex-2'></a>    \n",
        "### Exercise 2 - DecoderLayer\n",
        "    \n",
        "Implement `DecoderLayer()` using the `call()` method\n",
        "    \n",
        "1. Block 1 is a multi-head attention layer with a residual connection, and look-ahead mask. Like in the `EncoderLayer`, Dropout is defined within the multi-head attention layer.\n",
        "2. Block 2 will take into account the output of the Encoder, so the multi-head attention layer will receive K and V from the encoder, and Q from the Block 1. You will then apply a normalization layer and a residual connection, just like you did before with the `EncoderLayer`.\n",
        "3. Finally, Block 3 is a feed forward neural network with dropout and normalization layers and a residual connection.\n",
        "    \n",
        "**Additional Hints:**\n",
        "* The first two blocks are fairly similar to the EncoderLayer except you will return `attention_scores` when computing self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ldLKQXYo95s"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: DecoderLayer\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The decoder layer is composed by two multi-head attention blocks,\n",
        "    one that takes the new input and uses self-attention, and the other\n",
        "    one that combines it with the output of the encoder, followed by a\n",
        "    fully connected block.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dim,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.mha2 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embedding_dim,\n",
        "            dropout=dropout_rate\n",
        "        )\n",
        "\n",
        "        self.ffn = FullyConnected(\n",
        "            embedding_dim=embedding_dim,\n",
        "            fully_connected_dim=fully_connected_dim\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Decoder Layer\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            enc_output (tf.Tensor): Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
        "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            out3 (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            attn_weights_block1 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, target_seq_len)\n",
        "            attn_weights_block2 (tf.Tensor): Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "#         enc_output.shape == (batch_size, input_seq_len, fully_connected_dim)\n",
        "\n",
        "        # BLOCK 1\n",
        "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
        "        # Dropout will be applied during training (~1 line).\n",
        "        mult_attn_out1, attn_weights_block1 = self.mha1(\n",
        "            query= x, key= x, value= x, attention_mask=look_ahead_mask, return_attention_scores=True\n",
        "        )\n",
        "\n",
        "\n",
        "        # apply layer normalization (layernorm1) to the sum of the attention output and the input (~1 line)\n",
        "        Q1 = self.layernorm1(mult_attn_out1 + x)\n",
        "\n",
        "        # BLOCK 2\n",
        "        # calculate self-attention using the Q from the first block and K and V from the encoder output.\n",
        "        # Dropout will be applied during training\n",
        "        # Return attention scores as attn_weights_block2 (~1 line)\n",
        "        mult_attn_out2, attn_weights_block2 = self.mha2(\n",
        "            query= Q1, key= enc_output, value= enc_output , attention_mask=padding_mask, return_attention_scores=True\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # # apply layer normalization (layernorm2) to the sum of the attention output and the Q from the first block (~1 line)\n",
        "        mult_attn_out2 = self.layernorm2(mult_attn_out2 + Q1)\n",
        "\n",
        "        #BLOCK 3\n",
        "        # pass the output of the second block through a ffn\n",
        "        ffn_output = self.ffn(mult_attn_out2)\n",
        "\n",
        "        # apply a dropout layer to the ffn output\n",
        "        # use `training=training`\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "\n",
        "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block\n",
        "        out3 = self.layernorm2(ffn_output + mult_attn_out2)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvVdr9RKqfyy"
      },
      "source": [
        "<a name='7-2'></a>\n",
        "### Full Decoder\n",
        "You're almost there! Time to use your Decoder layer to build a full Transformer Decoder (Figure 3b). You will embed your output and add positional encodings. You will then feed your encoded embeddings to a stack of Decoder layers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<a name='ex-3'></a>     \n",
        "### Exercise 3 - Decoder\n",
        "\n",
        "Implement `Decoder()` using the `call()` method to embed your output, add positional encoding, and implement multiple decoder layers.\n",
        "\n",
        "In this exercise, you will initialize your Decoder with an Embedding layer, positional encoding, and multiple DecoderLayers. Your `call()` method will perform the following steps:\n",
        "1. Pass your generated output through the Embedding layer.\n",
        "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `tf.float32` before computing the square root.\n",
        "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
        "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode.\n",
        "5. Pass the output of the dropout layer through the stack of Decoding layers using a for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDGnQ6WKqjU1"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: Decoder\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the target input to an embedding layer\n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    decoder Layers\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps)\n",
        "                           for _ in range(self.num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward  pass for the Decoder\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
        "            enc_output (tf.Tensor):  Tensor of shape(batch_size, input_seq_len, fully_connected_dim)\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
        "            padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            x (tf.Tensor): Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # create word embeddings\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # scale embeddings by multiplying by the square root of their dimension\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "\n",
        "        # add positional encodings to word embedding\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        # apply a dropout layer to x\n",
        "        # use `training=training`\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
        "        for i in range(self.num_layers):\n",
        "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
        "            # of block 1 and 2 (~1 line)\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
        "\n",
        "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
        "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, fully_connected_dim)\n",
        "        return x, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyl-qvDiqm36",
        "outputId": "fc882e4c-fcf2-4a45-98ec-4b709b4f23d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using num_layers=7, embedding_dim=15 and num_heads=19:\n",
            "\n",
            "x has shape:(3, 4)\n",
            "Output of encoder has shape:(3, 7, 9)\n",
            "\n",
            "Output of decoder has shape:(3, 4, 15)\n",
            "\n",
            "Attention weights:\n",
            "decoder_layer1_block1_self_att has shape:(3, 19, 4, 4)\n",
            "decoder_layer1_block2_decenc_att has shape:(3, 19, 4, 7)\n",
            "decoder_layer2_block1_self_att has shape:(3, 19, 4, 4)\n",
            "decoder_layer2_block2_decenc_att has shape:(3, 19, 4, 7)\n",
            "decoder_layer3_block1_self_att has shape:(3, 19, 4, 4)\n",
            "decoder_layer3_block2_decenc_att has shape:(3, 19, 4, 7)\n",
            "decoder_layer4_block1_self_att has shape:(3, 19, 4, 4)\n",
            "decoder_layer4_block2_decenc_att has shape:(3, 19, 4, 7)\n",
            "decoder_layer5_block1_self_att has shape:(3, 19, 4, 4)\n",
            "decoder_layer5_block2_decenc_att has shape:(3, 19, 4, 7)\n",
            "decoder_layer6_block1_self_att has shape:(3, 19, 4, 4)\n",
            "decoder_layer6_block2_decenc_att has shape:(3, 19, 4, 7)\n",
            "decoder_layer7_block1_self_att has shape:(3, 19, 4, 4)\n",
            "decoder_layer7_block2_decenc_att has shape:(3, 19, 4, 7)\n"
          ]
        }
      ],
      "source": [
        "# Test your function!\n",
        "n_layers = 7\n",
        "emb_d = 15\n",
        "n_heads = 19\n",
        "fully_connected_dim = 16\n",
        "target_vocab_size = 300\n",
        "maximum_position_encoding = 6\n",
        "\n",
        "x = np.array([[1, 2, 1, 1], [0, 1, 1, 0], [2, 1, 1, 0]])\n",
        "\n",
        "encoder_test_output = tf.convert_to_tensor(np.random.rand(3, 7, 9))\n",
        "\n",
        "look_ahead_mask = create_look_ahead_mask(x.shape[1])\n",
        "\n",
        "decoder_test = Decoder(n_layers, emb_d, n_heads, fully_connected_dim, target_vocab_size,maximum_position_encoding)\n",
        "\n",
        "outd, att_weights = decoder_test( x = x, enc_output = encoder_test_output, training = False,\n",
        "           look_ahead_mask = look_ahead_mask, padding_mask = None)\n",
        "\n",
        "print(f\"Using num_layers={n_layers}, embedding_dim={emb_d} and num_heads={n_heads}:\\n\")\n",
        "print(f\"x has shape:{x.shape}\")\n",
        "print(f\"Output of encoder has shape:{encoder_test_output.shape}\\n\")\n",
        "\n",
        "print(f\"Output of decoder has shape:{outd.shape}\\n\")\n",
        "print(\"Attention weights:\")\n",
        "for name, tensor in att_weights.items():\n",
        "    print(f\"{name} has shape:{tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "201lPUCcsDNh"
      },
      "source": [
        "<a name='8'></a>\n",
        "## PART A.5 - Transformer\n",
        "\n",
        "Phew! This has been quite the assignment! Congratulations! You've done all the hard work, now it's time to put it all together.  \n",
        "\n",
        "\n",
        "The flow of data through the Transformer Architecture is as follows:\n",
        "* First your input passes through an Encoder, which is just repeated Encoder layers that you implemented:\n",
        "    - embedding and positional encoding of your input\n",
        "    - multi-head attention on your input\n",
        "    - feed forward neural network to help detect features\n",
        "* Then the predicted output passes through a Decoder, consisting of the decoder layers that you implemented:\n",
        "    - embedding and positional encoding of the output\n",
        "    - multi-head attention on your generated output\n",
        "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
        "    - a feed forward neural network to help detect features\n",
        "* Finally, after the Nth Decoder layer, one dense layer and a softmax are applied to generate prediction for the next output in your sequence.\n",
        "\n",
        "<a name='ex-4'></a>\n",
        "### Exercise 4 - Transformer\n",
        "\n",
        "Implement `Transformer()` using the `call()` method\n",
        "1. Pass the input through the Encoder with the appropiate mask.\n",
        "2. Pass the encoder output and the target through the Decoder with the appropiate mask.\n",
        "3. Apply a linear transformation and a softmax to get a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ5ZtZAsr-K4"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Complete transformer with an Encoder and a Decoder\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
        "               target_vocab_size, max_positional_encoding_input,\n",
        "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers=num_layers,\n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               input_vocab_size=input_vocab_size,\n",
        "                               maximum_position_encoding=max_positional_encoding_input,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.decoder = Decoder(num_layers=num_layers,\n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               target_vocab_size=target_vocab_size,\n",
        "                               maximum_position_encoding=max_positional_encoding_target,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire Transformer\n",
        "        Arguments:\n",
        "            input_sentence (tf.Tensor): Tensor of shape (batch_size, input_seq_len)\n",
        "                              An array of the indexes of the words in the input sentence\n",
        "            output_sentence (tf.Tensor): Tensor of shape (batch_size, target_seq_len)\n",
        "                              An array of the indexes of the words in the output sentence\n",
        "            training (bool): Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            enc_padding_mask (tf.Tensor): Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "            look_ahead_mask (tf.Tensor): Boolean mask for the target_input\n",
        "            dec_padding_mask (tf.Tensor): Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            final_output (tf.Tensor): The final output of the model\n",
        "            attention_weights (dict[str: tf.Tensor]): Dictionary of tensors containing all the attention weights for the decoder\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "\n",
        "        \"\"\"\n",
        "        ### START CODE HERE ###\n",
        "        # call self.encoder with the appropriate arguments to get the encoder output\n",
        "        enc_output = self.encoder(x=input_sentence, training=training, mask=enc_padding_mask)  # shape (batch_size, input_seq_len, embedding_dim)\n",
        "\n",
        "        # call self.decoder with the appropriate arguments to get the decoder output\n",
        "        # dec_output.shape == (batch_size, target_seq_len, embedding_dim)\n",
        "        dec_output, attention_weights = self.decoder(x=output_sentence, enc_output=enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask= dec_padding_mask)\n",
        "\n",
        "        # pass decoder output through a linear layer and softmax\n",
        "        final_output = self.final_layer(dec_output)  # shape (batch_size, target_seq_len, target_vocab_size)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w86ANTLsJeF",
        "outputId": "131092fe-2b50-4b01-d074-791496765694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_7', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_8', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_9', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_10', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_11', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_13', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using num_layers=7, target_vocab_size=350 and num_heads=19:\n",
            "\n",
            "sentence_a has shape:(1, 6)\n",
            "sentence_b has shape:(1, 6)\n",
            "\n",
            "Output of transformer (summary) has shape:(1, 6, 350)\n",
            "\n",
            "Attention weights:\n",
            "decoder_layer1_block1_self_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer1_block2_decenc_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer2_block1_self_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer2_block2_decenc_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer3_block1_self_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer3_block2_decenc_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer4_block1_self_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer4_block2_decenc_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer5_block1_self_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer5_block2_decenc_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer6_block1_self_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer6_block2_decenc_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer7_block1_self_att has shape:(1, 19, 6, 6)\n",
            "decoder_layer7_block2_decenc_att has shape:(1, 19, 6, 6)\n"
          ]
        }
      ],
      "source": [
        "# Test your function!\n",
        "n_layers = 7\n",
        "emb_d = 13\n",
        "n_heads = 19\n",
        "fully_connected_dim = 8\n",
        "input_vocab_size = 300\n",
        "target_vocab_size = 350\n",
        "max_positional_encoding_input = 12\n",
        "max_positional_encoding_target = 12\n",
        "\n",
        "transformer = Transformer(n_layers,\n",
        "    emb_d,\n",
        "    n_heads,\n",
        "    fully_connected_dim,\n",
        "    input_vocab_size,\n",
        "    target_vocab_size,\n",
        "    max_positional_encoding_input,\n",
        "    max_positional_encoding_target)\n",
        "\n",
        "# 0 is the padding value\n",
        "sentence_a = np.array([[1, 3, 0, 3, 0, 0]])\n",
        "sentence_b = np.array([[1, 0, 4, 0, 1, 0]])\n",
        "\n",
        "enc_padding_mask = create_padding_mask(sentence_a)\n",
        "dec_padding_mask = create_padding_mask(sentence_a)\n",
        "\n",
        "look_ahead_mask = create_look_ahead_mask(sentence_a.shape[1])\n",
        "\n",
        "test_summary, att_weights = transformer(\n",
        "    input_sentence=sentence_a,\n",
        "    output_sentence=sentence_b,\n",
        "    training=False,  # Pass training as a keyword argument\n",
        "    enc_padding_mask=enc_padding_mask,\n",
        "    look_ahead_mask=look_ahead_mask,\n",
        "    dec_padding_mask=dec_padding_mask\n",
        ")\n",
        "\n",
        "print(f\"Using num_layers={n_layers}, target_vocab_size={target_vocab_size} and num_heads={n_heads}:\\n\")\n",
        "print(f\"sentence_a has shape:{sentence_a.shape}\")\n",
        "print(f\"sentence_b has shape:{sentence_b.shape}\")\n",
        "\n",
        "print(f\"\\nOutput of transformer (summary) has shape:{test_summary.shape}\\n\")\n",
        "print(\"Attention weights:\")\n",
        "for name, tensor in att_weights.items():\n",
        "    print(f\"{name} has shape:{tensor.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_57wbkitMm7"
      },
      "source": [
        "<a name='9'></a>\n",
        "## PART B.1: Model Training - Initialize the Model\n",
        "Now that you have defined the model, you can initialize and train it. First you can initialize the model with the parameters below. Note that generally these models are much larger and you are using a smaller version to fit this environment and to be able to train it in just a few minutes.\n",
        "\n",
        "The base model described in the original Transformer paper used `num_layers=6`, `embedding_dim=512`, and `fully_connected_dim=2048`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmlPGcCutPSy"
      },
      "outputs": [],
      "source": [
        "# Define the model parameters\n",
        "num_layers = 2\n",
        "embedding_dim = 128\n",
        "fully_connected_dim = 128\n",
        "num_heads = 2\n",
        "positional_encoding_length = 256\n",
        "\n",
        "# Initialize the model\n",
        "transformer = Transformer(\n",
        "    num_layers,\n",
        "    embedding_dim,\n",
        "    num_heads,\n",
        "    fully_connected_dim,\n",
        "    vocab_size,\n",
        "    vocab_size,\n",
        "    positional_encoding_length,\n",
        "    positional_encoding_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBqbuQ11tSnY"
      },
      "source": [
        "<a name='10'></a>\n",
        "## Prepare for Training the Model\n",
        "\n",
        "The original transformer paper uses Adam optimizer with custom learning rate scheduling, which we define in the cell below. This was empirically shown to produce faster convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6qQyAzEtWRq"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(embedding_dim)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb4crEKrtipH"
      },
      "source": [
        "Next, you set up the loss. Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.\n",
        "\n",
        "You will use the sparse categorical cross-entropy loss function (`tf.keras.losses.SparseCategoricalCrossentropy`) and set the parameter `from_logits` to False since the Transformer does not output raw logits since the last layer has a softmax activation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM-kljwHtlLG"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
        "\n",
        "def masked_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "# Here you will store the losses, so you can later plot them\n",
        "losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSCVB-yztVa-"
      },
      "source": [
        "Now you can define your custom training function. If you are not very advanced with tensorflow, you can understand this function as an alternative to using `model.compile()` and `model.fit()`, but with added extra flexibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS9ch-m_tqhi"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(model, inp, tar):\n",
        "    \"\"\"\n",
        "    One training step for the transformer\n",
        "    Arguments:\n",
        "        inp (tf.Tensor): Input data to summarize\n",
        "        tar (tf.Tensor): Target (summary)\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    # Create masks\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
        "    dec_padding_mask = create_padding_mask(inp) # Notice that both encoder and decoder padding masks are equal\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = model(\n",
        "            inp,\n",
        "            tar_inp,\n",
        "             training = True,\n",
        "            enc_padding_mask = enc_padding_mask,\n",
        "            look_ahead_mask = look_ahead_mask,\n",
        "            dec_padding_mask = dec_padding_mask\n",
        "        )\n",
        "\n",
        "\n",
        "        loss = masked_loss(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0jZ25cntuR6"
      },
      "source": [
        "Now you are ready for training the model. But before starting the training, you can also define one more set of functions to perform the inference. Because you are using a custom training loop, you can do whatever you want between the training steps. And wouldnt't it be fun to see after each epoch some examples of how the model performs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avf4eryEtxDO"
      },
      "source": [
        "<a name='11'></a>\n",
        "## Summarization\n",
        "\n",
        "The last thing you will implement is inference. With this, you will be able to produce actual summaries of the documents. You will use a simple method called greedy decoding, which means you will predict one word at a time and append it to the output. You will start with an `[SOS]` token and repeat the word by word inference until the model returns you the `[EOS]` token or until you reach the maximum length of the sentence (you need to add this limit, otherwise a poorly trained model could give you infinite sentences without ever producing the `[EOS]` token.\n",
        "\n",
        "<a name='ex-5'></a>\n",
        "### Exercise 5 - next_word\n",
        "Write a helper function that predicts the next word, so you can use it to write the whole sentences. Hint: this is very similar to what happens in the train_step, but you have to set the training of the model to False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbmouEtNtzUQ"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: next_word\n",
        "def next_word(model, encoder_input, output):\n",
        "    \"\"\"\n",
        "    Helper function for summarization that uses the model to predict just the next word.\n",
        "    Arguments:\n",
        "        encoder_input (tf.Tensor): Input data to summarize\n",
        "        output (tf.Tensor): (incomplete) target (summary)\n",
        "    Returns:\n",
        "        predicted_id (tf.Tensor): The id of the predicted word\n",
        "    \"\"\"\n",
        "    # Create a padding mask for the input (encoder)\n",
        "    enc_padding_mask = create_padding_mask(encoder_input)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(output)[1])\n",
        "    dec_padding_mask = create_padding_mask(encoder_input)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Run the prediction of the next word with the transformer model\n",
        "    predictions, attention_weights = model(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            training=False,\n",
        "            enc_padding_mask=enc_padding_mask,  # Changed here\n",
        "            look_ahead_mask = look_ahead_mask,  # Changed here\n",
        "            dec_padding_mask=dec_padding_mask  # Changed here\n",
        "    )\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    predictions = predictions[: ,-1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    return predicted_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZt4jUPjt2Rp"
      },
      "source": [
        "Check if your function works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtugxmiCt4Xx",
        "outputId": "c642d07f-f2de-4d02-f8ea-5d87cf2441b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 2, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_15', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted token: [[9069]]\n",
            "Predicted word: liner\n"
          ]
        }
      ],
      "source": [
        "# Take a random sentence as an input\n",
        "input_document = tokenizer.texts_to_sequences([\"a random sentence\"])\n",
        "input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "# Take the start of sentence token as the only token in the output to predict the next word\n",
        "output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0)\n",
        "\n",
        "# predict the next word with your function\n",
        "predicted_token = next_word(transformer, encoder_input, output)\n",
        "print(f\"Predicted token: {predicted_token}\")\n",
        "\n",
        "predicted_word = tokenizer.sequences_to_texts(predicted_token.numpy())[0]\n",
        "print(f\"Predicted word: {predicted_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYMs6-JguSZy"
      },
      "outputs": [],
      "source": [
        "def summarize(model, input_document):\n",
        "    \"\"\"\n",
        "    A function for summarization using the transformer model\n",
        "    Arguments:\n",
        "        input_document (tf.Tensor): Input data to summarize\n",
        "    Returns:\n",
        "        _ (str): The summary of the input_document\n",
        "    \"\"\"\n",
        "    input_document = tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    output = tf.expand_dims([tokenizer.word_index[\"[SOS]\"]], 0)\n",
        "\n",
        "    for i in range(decoder_maxlen):\n",
        "        predicted_id = next_word(model, encoder_input, output)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        if predicted_id == tokenizer.word_index[\"[EOS]\"]:\n",
        "            break\n",
        "\n",
        "    return tokenizer.sequences_to_texts(output.numpy())[0]  # since there is just one translated document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI3tgF1Dua5X"
      },
      "source": [
        "Now you can already summarize a sentence! But beware, since the model was not yet trained at all, it will just produce nonsense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "kUGr_gDBufKT",
        "outputId": "22b02ec4-e301-4fd0-a691-3aed205fa1e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set example:\n",
            "[SOS] #person1#: hi, mr. smith. i'm doctor hawkins. why are you here today? #person2#: i found it would be a good idea to get a check-up. #person1#: yes, well, you haven't had one for 5 years. you should have one every year. #person2#: i know. i figure as long as there is nothing wrong, why go see the doctor? #person1#: well, the best way to avoid serious illnesses is to find out about them early. so try to come at least once a year for your own good. #person2#: ok. #person1#: let me see here. your eyes and ears look fine. take a deep breath, please. do you smoke, mr. smith? #person2#: yes. #person1#: smoking is the leading cause of lung cancer and heart disease, you know. you really should quit. #person2#: i've tried hundreds of times, but i just can't seem to kick the habit. #person1#: well, we have classes and some medications that might help. i'll give you more information before you leave. #person2#: ok, thanks doctor. [EOS]\n",
            "\n",
            "Human written summary:\n",
            "[SOS] mr. smith's getting a check-up, and doctor hawkins advises him to have one every year. hawkins'll give some information about their classes and medications to help mr. smith quit smoking. [EOS]\n",
            "\n",
            "Model written summary:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[SOS] liner hartsfield mate' marked campbell multicolor punished queuing envious sydemy guitar's prescribe destiny lend lend dolls tokyo somebody lumpur lend lend stationery vegas frank's booths xiaohui's xiaohui's xiaohui's perseverance perseverance perseverance perseverance perseverance brookston brookston blaze blaze blaze blaze blaze blaze blaze markheed unfilled 3s 3s 3s 3s 3s 3s\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "training_set_example = 0\n",
        "\n",
        "# Check a summary of a document from the training set\n",
        "print('Training set example:')\n",
        "print(document[training_set_example])\n",
        "print('\\nHuman written summary:')\n",
        "print(summary[training_set_example])\n",
        "print('\\nModel written summary:')\n",
        "summarize(transformer, document[training_set_example])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNlj1gNNuj7h"
      },
      "source": [
        "<a name='12'></a>\n",
        "# Train the model\n",
        "\n",
        "Now you can finally train the model. Below is a loop that will train your model for 20 epochs. note that it should take about 30 seconds per epoch (with the exception of the first few epochs which can take a few minutes each).\n",
        "\n",
        "Note that after each epoch you perform the summarization on one of the sentences in the test set and print it out, so you can see how your model is improving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubi6spIdumki",
        "outputId": "73da7871-c525-4b44-d79b-f51a26f4b7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 7.5806\n",
            "Time taken for one epoch: 38.467376947402954 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 person1 person1 person1 person1 person1 person1 person1 person1 the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "Epoch 2, Loss 6.0722\n",
            "Time taken for one epoch: 17.143752813339233 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 tells person1 to the to the to the to the and person2 's and person2 's and person2 's and person2 's [EOS]\n",
            "\n",
            "Epoch 3, Loss 5.5586\n",
            "Time taken for one epoch: 17.410478353500366 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 tells person1 to the new job and person2 's new job and person2 's new job and person2 's new job and person2 's new job [EOS]\n",
            "\n",
            "Epoch 4, Loss 5.2431\n",
            "Time taken for one epoch: 17.64471459388733 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 tells person1 the new job and person2 's new job and person2 's new job and the new job and the new job and the new job [EOS]\n",
            "\n",
            "Epoch 5, Loss 5.0382\n",
            "Time taken for one epoch: 17.776368141174316 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 and person2 are talking about the new job and person2 's assistance [EOS]\n",
            "\n",
            "Epoch 6, Loss 4.8821\n",
            "Time taken for one epoch: 17.798686265945435 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 and person2 are talking about the new job and tells person1 that he is a new york and the new york is a new york and the new york [EOS]\n",
            "\n",
            "Epoch 7, Loss 4.7583\n",
            "Time taken for one epoch: 17.805960178375244 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 and person2 are talking about the new york and tells person1 that he is a new york and the new york are going to the new york [EOS]\n",
            "\n",
            "Epoch 8, Loss 4.6463\n",
            "Time taken for one epoch: 17.905985116958618 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 tells person1 that he is a new york to go to the new york and the way to the way to the way to the way to the way [EOS]\n",
            "\n",
            "Epoch 9, Loss 4.5444\n",
            "Time taken for one epoch: 17.879555225372314 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 tells person1 that he is a new york to be a lot of the same day and the new york is a good job [EOS]\n",
            "\n",
            "Epoch 10, Loss 4.4482\n",
            "Time taken for one epoch: 18.081681966781616 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks mr smith to go to the airport and tells him the weather is a lot of the weather and the weather is a lot of the way to the way [EOS]\n",
            "\n",
            "Epoch 11, Loss 4.3541\n",
            "Time taken for one epoch: 17.851465225219727 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks mr smith to go to the airport and asks him to be a new york and asks him to be a lot of the same way to be a good idea [EOS]\n",
            "\n",
            "Epoch 12, Loss 4.2639\n",
            "Time taken for one epoch: 17.846248626708984 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks mr smith to go to the airport and asks him to be a lot of the way to be a lot of the way to the way to the way to the way [EOS]\n",
            "\n",
            "Epoch 13, Loss 4.1787\n",
            "Time taken for one epoch: 18.086069345474243 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks mr smith to go to the airport and asks him to be a lot of the city mr smith tells him that he is a lot of the way to be a lot of the way [EOS]\n",
            "\n",
            "Epoch 14, Loss 4.0960\n",
            "Time taken for one epoch: 17.88661241531372 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks mr smith to go to the airport and tells him that she is a lot of the city and the way to be a lot of the way to be a week [EOS]\n",
            "\n",
            "Epoch 15, Loss 4.0163\n",
            "Time taken for one epoch: 17.856521368026733 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks mr smith to go to the airport and tells him about the details of the way to the way to the way to the way to the way [EOS]\n",
            "\n",
            "Epoch 16, Loss 3.9418\n",
            "Time taken for one epoch: 17.942918062210083 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks mr smith to go to the office to the world with her husband for a few days to be a lot of the world and the end of the end of the contract [EOS]\n",
            "\n",
            "Epoch 17, Loss 3.8655\n",
            "Time taken for one epoch: 17.854214668273926 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks ms smith to go to the airport and tells him about the details of the world and how to go to the world with her [EOS]\n",
            "\n",
            "Epoch 18, Loss 3.7950\n",
            "Time taken for one epoch: 17.84090542793274 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks ms smith to go to the airport and asks him to go to the interview with her family and tells him that he can be a few days to be careful about the details [EOS]\n",
            "\n",
            "Epoch 19, Loss 3.7243\n",
            "Time taken for one epoch: 17.857383012771606 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks mrs smith to go to the office to visit her family and tells him that he can be a tour of the end of the office and he will be a few days [EOS]\n",
            "\n",
            "Epoch 20, Loss 3.6565\n",
            "Time taken for one epoch: 17.867676258087158 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks ms smith to help him with a letter to visit her husband and tells him that she can be a tour to be careful about the details of the details [EOS]\n",
            "\n",
            "Epoch 21, Loss 3.5900\n",
            "Time taken for one epoch: 17.94463300704956 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks ms smith to take a letter to the office to visit to the office and how to get some money from the office to make an agreement on saturday afternoon [EOS]\n",
            "\n",
            "Epoch 22, Loss 3.5248\n",
            "Time taken for one epoch: 17.87971782684326 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks ms green to take a letter to the office to visit his office and introduces the office to person1 and asks him to be careful about the details of the office [EOS]\n",
            "\n",
            "Epoch 23, Loss 3.4584\n",
            "Time taken for one epoch: 17.870334148406982 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks ms green to take a letter to london and introduces the details of the details of them to person1 about how to make an appointment with her family members [EOS]\n",
            "\n",
            "Epoch 24, Loss 3.3976\n",
            "Time taken for one epoch: 17.988394260406494 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks ms liu to help him with a letter of september introduces the details of the details of them to person1 about how to use the office and how to do some money [EOS]\n",
            "\n",
            "Epoch 25, Loss 3.3347\n",
            "Time taken for one epoch: 17.8840594291687 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks ms green to take a letter to paris to australia and tells him about the details of the details of the details of the office and how to do some money from herself [EOS]\n",
            "\n",
            "Epoch 26, Loss 3.2715\n",
            "Time taken for one epoch: 17.84615993499756 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] ms green tells person1 how to take a letter to the office and she wants to be a letter of them to be careful about the office and how to use of them [EOS]\n",
            "\n",
            "Epoch 27, Loss 3.2118\n",
            "Time taken for one epoch: 17.910223245620728 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] ms yang wants to take an interview to paris to the office supplies for a few days to help him with dr green they think about the office and how to use of them [EOS]\n",
            "\n",
            "Epoch 28, Loss 3.1531\n",
            "Time taken for one epoch: 17.836153984069824 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] person1 asks ms collins to take a letter to paris and tells him that he can be a letter and she can be a letter for advice on campus that she can be a letter from june [EOS]\n",
            "\n",
            "Epoch 29, Loss 3.0948\n",
            "Time taken for one epoch: 17.88957691192627 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] ms yang wants to take an international office to australia and tells him about the details of the office and how to use of the office to apply for advice [EOS]\n",
            "\n",
            "Epoch 30, Loss 3.0345\n",
            "Time taken for one epoch: 17.88020968437195 sec\n",
            "Example summarization on the test set:\n",
            "  True summarization:\n",
            "    [SOS] ms. dawson helps #person1# to write a memo to inform every employee that they have to change the communication method and should not use instant messaging anymore. in order to prevent employees from wasting time on instant message programs, #person1# decides to terminate the use of those programs and asks ms. dawson to send out a memo to all employees by the afternoon. ms. dawson takes a dictation for #person1# about prohibiting the use of instant message programs in the office. they argue about its reasonability but #person1# still insists. [EOS]\n",
            "  Predicted summarization:\n",
            "    [SOS] ms yang wants to take an interview to paris to the office by air pollution and tells him about the details of them they can use them to apply for advice [EOS]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take an example from the test set, to monitor it during training\n",
        "test_example = 0\n",
        "true_summary = summary_test[test_example]\n",
        "true_document = document_test[test_example]\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 30\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    start = time.time()\n",
        "    train_loss.reset_state()\n",
        "    number_of_batches=len(list(enumerate(dataset)))\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        print(f'Epoch {epoch+1}, Batch {batch+1}/{number_of_batches}', end='\\r')\n",
        "        train_step(transformer, inp, tar)\n",
        "\n",
        "    print (f'Epoch {epoch+1}, Loss {train_loss.result():.4f}')\n",
        "    losses.append(train_loss.result())\n",
        "\n",
        "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
        "    print('Example summarization on the test set:')\n",
        "    print('  True summarization:')\n",
        "    print(f'    {true_summary}')\n",
        "    print('  Predicted summarization:')\n",
        "    print(f'    {summarize(transformer, true_document)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBfZdON7zRYe"
      },
      "source": [
        "Plot the loss funtion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "aauXmmBJzULT",
        "outputId": "99838ca6-ad01-4cba-eb61-120925b22f0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(losses)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O43EWnprzXq3"
      },
      "source": [
        "<a name='13'></a>\n",
        "#  PART B.2:Summarize some Sentences!\n",
        "\n",
        "Below you can see an example of summarization of a sentence from the training set and a sentence from the test set. See if you notice anything interesting about them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShgGr3BSzdB3",
        "outputId": "7510d01d-d67a-476d-b32a-445242b8b8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set example:\n",
            "[SOS] #person1#: i'm tired of watching television. let's go to cinema to- night. #person2#: all right. do you want to go downtown? or is there a good movie in the neighborhood? #person1#: i'd rather not spend a lot of money. what does the pa- per say about neighborhood theaters? #person2#: here's the list on page... column 6. here it is. where's the rialto? there's a perfect movie there. #person1#: that's too far away. and it's hard to find a place to park there. #person2#: well, the grand theater has gone with the wind. #person1#: i saw that years ago. i couldn't wait to see it again. moreover, it's too long. we wouldn't get home until midnight. #person2#: the center has a horror film. you wouldn't want to see that? #person1#: no, indeed. i wouldn't be able to sleep tonight. #person2#: that's about ell there is. unless we change our decision and go downtown. #person1#: no, we just can't pay for it. there must be something else we haven't seen. #person2#: here, look for yourself, i can't find anything else. #person1#: look at this! #person2#: what? #person1#: in the television timetable, there's a baseball game on television tonight. #person2#: i wasn't looking for a tv program. i was looking at the movie ads. #person1#: i know, but i just happened to notice it. new york is playing boston. #person2#: that must be good. i wouldn't mind watching that. #person1#: ok. let's stay home. we can go to the cinema friday. [EOS]\n",
            "\n",
            "Human written summary:\n",
            "[SOS] #person1#'s tired of watching television, so #person1# and #person2# search on the paper to choose a movie to watch. but they don't decide a suitable one. #person1# finds there will be a baseball game tonight, so they decide to stay at home. [EOS]\n",
            "\n",
            "Model written summary:\n",
            "[SOS] person1 and person2 are planning to go to the movie but they are going to see a movie but they are going to go to the cinema but person2 refuses [EOS]\n"
          ]
        }
      ],
      "source": [
        "training_set_example = 35\n",
        "\n",
        "# Check a summary of a document from the training set\n",
        "print('Training set example:')\n",
        "print(document[training_set_example])\n",
        "print('\\nHuman written summary:')\n",
        "print(summary[training_set_example])\n",
        "print('\\nModel written summary:')\n",
        "print(summarize(transformer, document[training_set_example]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0ieA4BPze28",
        "outputId": "dfba4258-2892-4df5-de1d-c58931940e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set example:\n",
            "[SOS] #person1#: there's a car waiting for you just outside the door. right this way, please. #person2#: ok! #person1#: let me put your cases into the trunk and please get in the back. #person2#: thanks! #person1#: how was your flight? #person2#: it's comfortable, but now i'm a little tired. #person1#: we'll reach the beijing hotel in another ten minutes. when we arrived there, you can go up and have a rest. the hotel has very good service, and it's considered as one of the best hotels here. #person2#: thank you! i lived there when i came to beijing last time. it's comfortable and beautiful. #person1#: if it's convenient for you, mr. wu would like to invite you to the banquet in honor of you in the evening. #person2#: thank you! i will. when and where will the dinner be? #person1#: at six o'clock in the international hotel. we'll pick you up this afternoon. besides, if you care for visiting, we'll arrange some sightseeing for you. #person2#: oh, that's nice. thank you for arranging all of this. [EOS]\n",
            "\n",
            "Human written summary:\n",
            "[SOS] #person1# is driving #person2# to the beijing hotel. #person2# will attend a banquet at six o'clock in the international hotel. #person1# warmly welcomes #person2# and drives #person2# to the beijing hotel. mr. wu has arranged a banquet for #person2# in the evening. #person1# has arranged everything for #person2# after #person2# arrives in beijing and invites #person2# to a banquet. [EOS]\n",
            "\n",
            "Model written summary:\n",
            "[SOS] mr black tells person1 the way to the airport and the hotel is going to the hotel in beijing person1 asks mr black to come in the hotel for the hotel [EOS]\n"
          ]
        }
      ],
      "source": [
        "test_set_example = 83\n",
        "\n",
        "# Check a summary of a document from the test set\n",
        "print('Test set example:')\n",
        "print(document_test[test_set_example])\n",
        "print('\\nHuman written summary:')\n",
        "print(summary_test[test_set_example])\n",
        "print('\\nModel written summary:')\n",
        "print(summarize(transformer, document_test[test_set_example]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7jOZ7a1SdM3"
      },
      "source": [
        "## PART B.3: Calculate BERTScore\n",
        "There are several methods to calculate similarity between two texts. You will use BERTScore\n",
        "to measure the similarity between your summaries and the ground truth summaries given\n",
        "in the dataset. You can adjust the given code to calculate your score on the test set and\n",
        "training set. Share your average results for training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpezsTRSHg00",
        "outputId": "8bfa92dd-1710-41c9-95df-3304eb6c4ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472,
          "referenced_widgets": [
            "150ac6604d864cfbba2441839fffa8d6",
            "2cc182352f804a5db1ef0e9676957597",
            "cfead449801146d5a1f91b08c63b4c27",
            "1791e68d8536443c842e1568108df70d",
            "5ca1feb54b8d46e0878fad16f8addb91",
            "ef80ec99d6804e868e0189a264f402d6",
            "0c2862c798694339ae03cbb7a9fe7d64",
            "d64edae389da471aa175b7a9c62cc830",
            "36ad4f5ee3c44d24866c14d7cf465212",
            "c998b3788c564608ab4767cad2eaa0ee",
            "b479487f011e4e34a62a3ee624b8665e",
            "732c9772be5d49fea86acba2e10618ff",
            "2e8a67d733ef41f98023f22d9bc9c3e1",
            "1fb742c569f943a380acab862fac24f8",
            "f499befc14794c8097f047c1e274b8fb",
            "baf673d096b44ffaa8fbe0320e45a3ae",
            "8ed851511da84dcfbedba8152e3e1eb0",
            "717badf01c934c5783fafd9782258bbd",
            "9864ca127f984ab8a7a0ef185719fcd3",
            "8fab043435aa4081b9c19b9057c77c71",
            "c9a73e5415ae409daf394f137fe04010",
            "7a0858e3c16046d8ae8011c969bc18b2",
            "75498b4d0b674992a66b1075f16306ca",
            "de0b0ead91c84c6ab8d1427cd0b0ac33",
            "088196f339e2427eb6992e631d3f1d1c",
            "28cbdfe1aa1b457f810978ccfd9fa90c",
            "1123a6cbe37e45cc91e02ead85badcee",
            "2364c3663513486ead05b84910cd9af1",
            "69bdbd33d45a4820b9234b23927fd273",
            "bd1e3e52e5ad4819980b832b3b3c3557",
            "f45fd01376e34ac1b941881c0bd871e2",
            "1f19522539504d24b43ff956c9fff2d8",
            "967ba9845b6f41988e6d6605d5dd23be",
            "7bb2e03fc51c4795ac41824a9c64b7c3",
            "4478a29a882f46d08497cddf96caa029",
            "abfcc84db5b0456b8871a112a282e66c",
            "3fbe24c142cf4b08921452353b46947c",
            "9ada24db33054368b85c748e8ca65c57",
            "d7dd9a4d95004f31b893445eb0048359",
            "881efeef4be64d54a1888b44a235594e",
            "229bfcd19ee2416883dba369af11b1ed",
            "121c771b9e194f1b915554d54ad27a06",
            "04c7c806da3348be8484178f0b8ab951",
            "62500a5f2eb44aaf95e9efc891424ea9",
            "9fc4560e6ef74f4ebf748102d9bd90dd",
            "4c6536402f8d495d96c0ba5e87636982",
            "392158a19c364bf4ba291cb6491871e6",
            "7c3da382aed44f608a17303372699672",
            "e9e96a4ecba94e5eb0a486b9b08a9412",
            "50b79dd4dd654fddb643644e6841cd0a",
            "211e4522ccc04227a3eee32d9b9eddc6",
            "96bb71fe38f143389fc4518a30686473",
            "052cfab169274d498e39618387cf46da",
            "0459aa2df6f14e2586744947f3f5814c",
            "af1ce082617d490db92db4c30199d2ce",
            "2752e7500e774fc59f1c5cc8f92fdf34",
            "8e795a80bdbc4248804bed1cc076aceb",
            "7dd0fa916b1643238c9b210749899b91",
            "cb1cd24c936e410c9dffa8156f93ca7e",
            "f3464cec1863465180bbdea9b8203fa1",
            "de9d9e438ebe490db9033d637cef4806",
            "f523feae638049fd8add628eba7bed99",
            "343438f78a2a4c068e0fbd6e45b6bd65",
            "0c474200c05449bf8f38ae8a8ee15c46",
            "f172361074ac44be91ad55400ca72530",
            "3db10e71f19c42ada58e01e53046ce37"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part b.3: Calculate BERTScore\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "150ac6604d864cfbba2441839fffa8d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "732c9772be5d49fea86acba2e10618ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75498b4d0b674992a66b1075f16306ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bb2e03fc51c4795ac41824a9c64b7c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fc4560e6ef74f4ebf748102d9bd90dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2752e7500e774fc59f1c5cc8f92fdf34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 2, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision: 0.8870293972492218\n",
            "Average Recall: 0.8474194389581681\n",
            "Average F1 Score: 0.8667053527832032\n"
          ]
        }
      ],
      "source": [
        "print('part b.3: Calculate BERTScore')\n",
        "from bert_score import score\n",
        "\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Initialize lists to store scores\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "# Loop through each pair of reference and candidate sentences\n",
        "for test_set_example, reference in zip(document_test, summary_test):\n",
        "    prediction = summarize(transformer, test_set_example)\n",
        "    P, R, F1 = score([prediction], [reference], lang=\"en\", model_type=\"roberta-large\")\n",
        "\n",
        "    # Append the scores\n",
        "    precision_scores.append(P.item())\n",
        "    recall_scores.append(R.item())\n",
        "    f1_scores.append(F1.item())\n",
        "\n",
        "# Calculate average scores\n",
        "avg_precision = sum(precision_scores) / len(precision_scores)\n",
        "avg_recall = sum(recall_scores) / len(recall_scores)\n",
        "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "# Print the average scores\n",
        "print(f\"Average Precision: {avg_precision}\")\n",
        "print(f\"Average Recall: {avg_recall}\")\n",
        "print(f\"Average F1 Score: {avg_f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "from transformers import logging\n",
        "import random\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# random sampling to 1/20 to avoid computational overhead\n",
        "random.seed(42)\n",
        "n = len(document)\n",
        "sample_size = n // 20\n",
        "sample_indices = random.sample(range(n), sample_size)\n",
        "sample_doc = [document[i] for i in sample_indices]\n",
        "sample_ref = [summary[i] for i in sample_indices]\n",
        "\n",
        "# Initialize lists to store scores\n",
        "precision_scores = []\n",
        "recall_scores    = []\n",
        "f1_scores        = []\n",
        "\n",
        "# Loop through each pair of reference and candidate sentences\n",
        "for training_set_example, reference in zip(sample_doc, sample_ref):\n",
        "    prediction = summarize(transformer, training_set_example)\n",
        "    P, R, F1 = score([prediction], [reference], lang=\"en\", model_type=\"roberta-large\")\n",
        "\n",
        "    # Append the scores\n",
        "    precision_scores.append(P.item())\n",
        "    recall_scores.append(R.item())\n",
        "    f1_scores.append(F1.item())\n",
        "\n",
        "avg_precision = sum(precision_scores) / len(precision_scores)\n",
        "avg_recall    = sum(recall_scores)    / len(recall_scores)\n",
        "avg_f1        = sum(f1_scores)        / len(f1_scores)\n",
        "\n",
        "print(f\"Sampled Training (1/20) – Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Sampled Training (1/20) – Average Recall:    {avg_recall:.4f}\")\n",
        "print(f\"Sampled Training (1/20) – Average F1 Score:  {avg_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-bXXzOt2pZ4",
        "outputId": "e6c48c29-30a4-4357-aeef-efe0f5d867d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 2, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled Training (1/20) – Average Precision: 0.9015\n",
            "Sampled Training (1/20) – Average Recall:    0.8848\n",
            "Sampled Training (1/20) – Average F1 Score:  0.8930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vsFPHSN0PNG"
      },
      "source": [
        "\n",
        "**ACKNOWLEDGEMENT**  \n",
        "This assignment is an adaptation from:\n",
        "Deeplearning.ai NLP Course and\n",
        "Dataset is from https://github.com/cylnlp/dialogsum"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "D7jOZ7a1SdM3"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "150ac6604d864cfbba2441839fffa8d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cc182352f804a5db1ef0e9676957597",
              "IPY_MODEL_cfead449801146d5a1f91b08c63b4c27",
              "IPY_MODEL_1791e68d8536443c842e1568108df70d"
            ],
            "layout": "IPY_MODEL_5ca1feb54b8d46e0878fad16f8addb91"
          }
        },
        "2cc182352f804a5db1ef0e9676957597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef80ec99d6804e868e0189a264f402d6",
            "placeholder": "​",
            "style": "IPY_MODEL_0c2862c798694339ae03cbb7a9fe7d64",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "cfead449801146d5a1f91b08c63b4c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d64edae389da471aa175b7a9c62cc830",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36ad4f5ee3c44d24866c14d7cf465212",
            "value": 25
          }
        },
        "1791e68d8536443c842e1568108df70d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c998b3788c564608ab4767cad2eaa0ee",
            "placeholder": "​",
            "style": "IPY_MODEL_b479487f011e4e34a62a3ee624b8665e",
            "value": " 25.0/25.0 [00:00&lt;00:00, 2.27kB/s]"
          }
        },
        "5ca1feb54b8d46e0878fad16f8addb91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef80ec99d6804e868e0189a264f402d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c2862c798694339ae03cbb7a9fe7d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d64edae389da471aa175b7a9c62cc830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36ad4f5ee3c44d24866c14d7cf465212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c998b3788c564608ab4767cad2eaa0ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b479487f011e4e34a62a3ee624b8665e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "732c9772be5d49fea86acba2e10618ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e8a67d733ef41f98023f22d9bc9c3e1",
              "IPY_MODEL_1fb742c569f943a380acab862fac24f8",
              "IPY_MODEL_f499befc14794c8097f047c1e274b8fb"
            ],
            "layout": "IPY_MODEL_baf673d096b44ffaa8fbe0320e45a3ae"
          }
        },
        "2e8a67d733ef41f98023f22d9bc9c3e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ed851511da84dcfbedba8152e3e1eb0",
            "placeholder": "​",
            "style": "IPY_MODEL_717badf01c934c5783fafd9782258bbd",
            "value": "config.json: 100%"
          }
        },
        "1fb742c569f943a380acab862fac24f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9864ca127f984ab8a7a0ef185719fcd3",
            "max": 482,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fab043435aa4081b9c19b9057c77c71",
            "value": 482
          }
        },
        "f499befc14794c8097f047c1e274b8fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9a73e5415ae409daf394f137fe04010",
            "placeholder": "​",
            "style": "IPY_MODEL_7a0858e3c16046d8ae8011c969bc18b2",
            "value": " 482/482 [00:00&lt;00:00, 44.5kB/s]"
          }
        },
        "baf673d096b44ffaa8fbe0320e45a3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed851511da84dcfbedba8152e3e1eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "717badf01c934c5783fafd9782258bbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9864ca127f984ab8a7a0ef185719fcd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fab043435aa4081b9c19b9057c77c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9a73e5415ae409daf394f137fe04010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a0858e3c16046d8ae8011c969bc18b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75498b4d0b674992a66b1075f16306ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de0b0ead91c84c6ab8d1427cd0b0ac33",
              "IPY_MODEL_088196f339e2427eb6992e631d3f1d1c",
              "IPY_MODEL_28cbdfe1aa1b457f810978ccfd9fa90c"
            ],
            "layout": "IPY_MODEL_1123a6cbe37e45cc91e02ead85badcee"
          }
        },
        "de0b0ead91c84c6ab8d1427cd0b0ac33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2364c3663513486ead05b84910cd9af1",
            "placeholder": "​",
            "style": "IPY_MODEL_69bdbd33d45a4820b9234b23927fd273",
            "value": "vocab.json: 100%"
          }
        },
        "088196f339e2427eb6992e631d3f1d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd1e3e52e5ad4819980b832b3b3c3557",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f45fd01376e34ac1b941881c0bd871e2",
            "value": 898823
          }
        },
        "28cbdfe1aa1b457f810978ccfd9fa90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f19522539504d24b43ff956c9fff2d8",
            "placeholder": "​",
            "style": "IPY_MODEL_967ba9845b6f41988e6d6605d5dd23be",
            "value": " 899k/899k [00:00&lt;00:00, 10.5MB/s]"
          }
        },
        "1123a6cbe37e45cc91e02ead85badcee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2364c3663513486ead05b84910cd9af1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69bdbd33d45a4820b9234b23927fd273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd1e3e52e5ad4819980b832b3b3c3557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f45fd01376e34ac1b941881c0bd871e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f19522539504d24b43ff956c9fff2d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "967ba9845b6f41988e6d6605d5dd23be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bb2e03fc51c4795ac41824a9c64b7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4478a29a882f46d08497cddf96caa029",
              "IPY_MODEL_abfcc84db5b0456b8871a112a282e66c",
              "IPY_MODEL_3fbe24c142cf4b08921452353b46947c"
            ],
            "layout": "IPY_MODEL_9ada24db33054368b85c748e8ca65c57"
          }
        },
        "4478a29a882f46d08497cddf96caa029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7dd9a4d95004f31b893445eb0048359",
            "placeholder": "​",
            "style": "IPY_MODEL_881efeef4be64d54a1888b44a235594e",
            "value": "merges.txt: 100%"
          }
        },
        "abfcc84db5b0456b8871a112a282e66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_229bfcd19ee2416883dba369af11b1ed",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_121c771b9e194f1b915554d54ad27a06",
            "value": 456318
          }
        },
        "3fbe24c142cf4b08921452353b46947c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04c7c806da3348be8484178f0b8ab951",
            "placeholder": "​",
            "style": "IPY_MODEL_62500a5f2eb44aaf95e9efc891424ea9",
            "value": " 456k/456k [00:00&lt;00:00, 2.73MB/s]"
          }
        },
        "9ada24db33054368b85c748e8ca65c57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7dd9a4d95004f31b893445eb0048359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "881efeef4be64d54a1888b44a235594e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "229bfcd19ee2416883dba369af11b1ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "121c771b9e194f1b915554d54ad27a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04c7c806da3348be8484178f0b8ab951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62500a5f2eb44aaf95e9efc891424ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fc4560e6ef74f4ebf748102d9bd90dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c6536402f8d495d96c0ba5e87636982",
              "IPY_MODEL_392158a19c364bf4ba291cb6491871e6",
              "IPY_MODEL_7c3da382aed44f608a17303372699672"
            ],
            "layout": "IPY_MODEL_e9e96a4ecba94e5eb0a486b9b08a9412"
          }
        },
        "4c6536402f8d495d96c0ba5e87636982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b79dd4dd654fddb643644e6841cd0a",
            "placeholder": "​",
            "style": "IPY_MODEL_211e4522ccc04227a3eee32d9b9eddc6",
            "value": "tokenizer.json: 100%"
          }
        },
        "392158a19c364bf4ba291cb6491871e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96bb71fe38f143389fc4518a30686473",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_052cfab169274d498e39618387cf46da",
            "value": 1355863
          }
        },
        "7c3da382aed44f608a17303372699672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0459aa2df6f14e2586744947f3f5814c",
            "placeholder": "​",
            "style": "IPY_MODEL_af1ce082617d490db92db4c30199d2ce",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 8.29MB/s]"
          }
        },
        "e9e96a4ecba94e5eb0a486b9b08a9412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50b79dd4dd654fddb643644e6841cd0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "211e4522ccc04227a3eee32d9b9eddc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96bb71fe38f143389fc4518a30686473": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "052cfab169274d498e39618387cf46da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0459aa2df6f14e2586744947f3f5814c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af1ce082617d490db92db4c30199d2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2752e7500e774fc59f1c5cc8f92fdf34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e795a80bdbc4248804bed1cc076aceb",
              "IPY_MODEL_7dd0fa916b1643238c9b210749899b91",
              "IPY_MODEL_cb1cd24c936e410c9dffa8156f93ca7e"
            ],
            "layout": "IPY_MODEL_f3464cec1863465180bbdea9b8203fa1"
          }
        },
        "8e795a80bdbc4248804bed1cc076aceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de9d9e438ebe490db9033d637cef4806",
            "placeholder": "​",
            "style": "IPY_MODEL_f523feae638049fd8add628eba7bed99",
            "value": "model.safetensors: 100%"
          }
        },
        "7dd0fa916b1643238c9b210749899b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_343438f78a2a4c068e0fbd6e45b6bd65",
            "max": 1421700479,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c474200c05449bf8f38ae8a8ee15c46",
            "value": 1421700479
          }
        },
        "cb1cd24c936e410c9dffa8156f93ca7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f172361074ac44be91ad55400ca72530",
            "placeholder": "​",
            "style": "IPY_MODEL_3db10e71f19c42ada58e01e53046ce37",
            "value": " 1.42G/1.42G [00:13&lt;00:00, 120MB/s]"
          }
        },
        "f3464cec1863465180bbdea9b8203fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9d9e438ebe490db9033d637cef4806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f523feae638049fd8add628eba7bed99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "343438f78a2a4c068e0fbd6e45b6bd65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c474200c05449bf8f38ae8a8ee15c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f172361074ac44be91ad55400ca72530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3db10e71f19c42ada58e01e53046ce37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}